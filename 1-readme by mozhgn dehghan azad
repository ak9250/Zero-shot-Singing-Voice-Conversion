In this paper, we propose the application of speaker embedding networks for zero-shot SVC. We suggest two architectures for carrying out zero-shot SVC using the WORLD
vocoder for modeling singing voice. Overall, we find that
speaker embeddings can indeed be used directly for zeroshot SVC. Moreover, zero-shot networks replacing onehot speaker labels with speaker embeddings perform as
well as (or even better than) their supervised closed set
counterparts, with the invaluable added benefits that they
can be trained on unlabeled data and can potentially adapt
to new voices without requiring further training. Furthermore, we show that there is some benefit to training zeroshot SVC networks by adapting an initial model trained on
large amounts of speech data. In future work, we will investigate learning latent factors which can allow for further
expressive manipulation of conversion results. While some
initial progress to this end has been made using Gaussian
Mixture VAEs (GMVAEs) [11], they have largely been
limited to sung vowels. We can likely generalize this to
more practical singing voice by utilizing the conditioning
signals used in this work. We are also interested in replacing the WORLD vocoder with learned vocoders based on
differentiable digital signal processing, as in [18, 25], in
order to enable lightweight end-to-end training
